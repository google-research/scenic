# Copyright 2024 The Scenic Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Utilities for PolyVit trainer."""

from typing import Any, Dict, Optional, Tuple

from absl import logging
from flax.training import checkpoints
import jax
import jax.numpy as jnp
import ml_collections
import numpy as np
from scenic.dataset_lib import datasets
from scenic.train_lib import train_utils
from tensorflow.io import gfile


def get_num_training_steps(
    config: ml_collections.ConfigDict,
    datasets_metadata: Dict[str, Dict[str, Any]]) -> Tuple[int, Optional[int]]:
  """Calculates the total number of training step and possibly steps_per_epoch.

  The main training loop is based on number of training steps. Thus, for
  datasets
  that we want to train based on number of epochs, we need to calculate the
  total number of training steps. This function looks for `num_training_steps`
  in config, if it exists it returns that as the total step and `None` as
  `steps_per_epoch`. If num_training_steps doesn't exist, then it looks for
  `num_training_epochs` and given the size of training data calculates the total
  steps and steps_per_epoch. In this computation, we assume that
  drop_remainder=True.

  Args:
    config: Configuration of the experiment.
    datasets_metadata: Meta-data that is generated by the dataset_builder.

  Returns:
    total_steps: Total number of training steps.
    steps_per_epoch: Number of steps in every epoch.
  """
  num_total_train_examples = 0
  for ds_metadata in datasets_metadata.values():
    num_total_train_examples += ds_metadata.get('num_train_examples', 0)

  # We either use num_training_epochs or num_training_steps.
  steps_per_epoch = num_total_train_examples // get_average_batch_size(config)

  if config.get('num_training_steps'):
    assert not config.get('num_training_epochs')
    return config.num_training_steps, steps_per_epoch or None
  else:
    assert config.num_training_epochs and not config.get('num_training_steps')
    return (steps_per_epoch * config.num_training_epochs), steps_per_epoch


def get_average_batch_size(config: ml_collections.ConfigDict):
  """Computes average batch size."""

  if config.get('batch_size') is not None:
    return config.batch_size

  batch_sizes_sum = 0
  n_datasets = 0

  for bs in config.batch_sizes.values():
    batch_sizes_sum += bs
    n_datasets += 1

  average_batch_size = int(batch_sizes_sum // n_datasets)

  return average_batch_size


def get_datasets(config: ml_collections.ConfigDict,
                 data_rng: jnp.ndarray,
                 dataset_service_address: Optional[str] = None):
  """Creates dataset from config."""

  device_count = jax.device_count()
  logging.info('device_count: %d', device_count)
  logging.info('num_hosts : %d', jax.process_count())
  logging.info('host_id : %d', jax.process_index())

  dataset_dict = {}
  for ds_name, cfg in config.datasets.items():

    # This key is needed for disabling datasets in hyperparameter sweeps.
    if cfg.get('dont_use') is not None and cfg.get('dont_use'):
      continue

    if config.get('batch_sizes') is not None:
      batch_size = config.batch_sizes.get(ds_name)
    else:
      batch_size = config.batch_size

    if batch_size % device_count > 0:
      raise ValueError(
          f'Batch size ({batch_size}) of {ds_name} must be divisible '
          f'by the number of devices ({device_count})')

    if config.get('eval_batch_sizes') is not None:
      eval_batch_size = config.eval_batch_sizes.get(ds_name)
    else:
      eval_batch_size = config.get('eval_batch_size', batch_size)

    if eval_batch_size % device_count > 0:
      raise ValueError(
          f'Eval batch size ({eval_batch_size}) of {ds_name} must be '
          f'divisible by the number of devices ({device_count})')

    local_batch_size = batch_size // jax.process_count()
    eval_local_batch_size = eval_batch_size // jax.process_count()
    device_batch_size = batch_size // device_count
    logging.info('local_batch_size of %s : %d', ds_name, local_batch_size)
    logging.info('device_batch_size of %s : %d', ds_name, device_batch_size)

    shuffle_seed = cfg.get('shuffle_seed', None)
    if dataset_service_address and shuffle_seed is not None:
      raise ValueError('Using dataset service with a random seed causes each '
                       'worker to produce exactly the same data. Add '
                       'config.shuffle_seed = None to your config if you want '
                       'to run with dataset service.')

    # 'bit' consists of many datasets, so we do this to have a unique dataset
    # key if we train on multiple datasets from 'bit'. E.g. ds_name =
    # 'bit_caltech'.
    if ds_name.startswith('bit_'):
      dataset_builder = datasets.get_dataset('bit')
    elif ds_name in ['kinetics400', 'moments_in_time']:
      dataset_builder = datasets.get_dataset('video_tfrecord_dataset')
    elif ds_name in ['vggsound', 'audioset']:
      dataset_builder = datasets.get_dataset('audiovisual_tfrecord_dataset')
    else:
      dataset_builder = datasets.get_dataset(ds_name)

    dataset_rng, data_rng = jax.random.split(data_rng)
    ds = dataset_builder(
        batch_size=local_batch_size,
        eval_batch_size=eval_local_batch_size,
        num_shards=jax.local_device_count(),
        dtype_str=cfg.data_dtype_str,
        rng=dataset_rng,
        shuffle_seed=shuffle_seed,
        dataset_configs=cfg,
        dataset_service_address=dataset_service_address)

    # Add task information to the dataset meta_data:
    ds.meta_data['task'] = cfg.task
    ds.meta_data['modality'] = cfg.get('modality', 'image')
    dataset_dict[ds_name] = ds

  return dataset_dict


def restore_pretrained_big_vision_checkpoint(
    checkpoint_path: str,
) -> train_utils.TrainState:
  """Loads and converts a big_vision checkpoint to a scenic train state.

  The model weights, global step and accumulated train time are extracted.
  Optimizer state, such as the momentum, is not extracted.

  Args:
    checkpoint_path: Path to big_vision checkpoint.

  Returns:
    restored_train_state: Scenic train state with model weights, global step
      and accumulated training time.
  """

  def unflatten_dict(
      flattened: Dict[str, Any], separator: str = '/', leaf_idx: int = -1
  ) -> Dict[str, Any]:
    unflattened = {}
    for k, v in flattened.items():
      subtree = unflattened
      if leaf_idx != 0:
        path = k.split(separator)[:leaf_idx]
      else:
        path = k.split(separator)
      for k2 in path[:-1]:
        if k2 not in subtree:
          subtree[k2] = {}
        subtree = subtree[k2]
      subtree[path[-1]] = v
    return unflattened

  logging.info('Loading big_vision checkpoint from %s', checkpoint_path)
  checkpoint_data = np.load(gfile.GFile(checkpoint_path, 'rb'))
  tree = unflatten_dict(checkpoint_data, separator='/', leaf_idx=0)

  restored_params = tree['opt']['target']
  restored_params = checkpoints.convert_pre_linen(restored_params)
  restored_params = dict(restored_params)
  train_state = train_utils.TrainState()
  # pytype: disable=wrong-arg-types
  restored_train_state = train_state.replace(  # pytype: disable=attribute-error
      params=restored_params
  )
  # pytype: enable=wrong-arg-types

  return restored_train_state
