{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PaiG8Ulc75xc"
      },
      "source": [
        "# ü¶â OWL-ViT inference playground\n",
        "\n",
        "OWL-ViT is an **open-vocabulary object detector**. Given a free-text query, it will find objects matching that query. It can also do **one-shot object detection**, i.e. detect objects based on a single example image.\n",
        "\n",
        "This Colab allows you to query the model interactively, to get a feeling for its capabilities. For details on the model, check out the [paper](https://arxiv.org/abs/2205.06230) or the [code](https://github.com/google-research/scenic/tree/main/scenic/projects/owl_vit).\n",
        "\n",
        "\u003e ‚ùó Note: The free public Colab runtime has enough memory for the ViT-B/16 model. For optimal results, use a Pro or local runtime and the ViT-L/14 model.\n",
        "\n",
        "\u003e ‚ùó Note: This Colab is optimized for fast interactive exploration. It does not apply some of the optimizations and augmentations that would be used in a rigorous evaluation settings, so results from this Colab may not match the paper.\n",
        "\n",
        "## How to use this Colab\n",
        "1. Use a GPU or TPU Colab runtime.\n",
        "2. Run all cells in the Colab from top to bottom.\n",
        "3. Go to the cells for [Text-conditioned object detection](#scrollTo=aNzcyP1sbJ9w\u0026uniqifier=1) or [Image-conditioned object detection](#scrollTo=TFlZhrDTQbiY\u0026uniqifier=1) and have fun!\n",
        "\n",
        "**If you run into any problems, please [file an issue](https://github.com/google-research/scenic/issues/new?title=OWL-ViT+inference+playround:+[add+title]) on GitHub.**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-Yta1B7rtWu"
      },
      "source": [
        "# Download and install OWL-ViT\n",
        "\n",
        "OWL-ViT is implemented in [Scenic](https://github.com/google-research/scenic). The cell below installs the Scenic codebase from GitHub and imports it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zWF7RkeZ4B_N"
      },
      "outputs": [],
      "source": [
        "!rm -rf *\n",
        "!rm -rf .config\n",
        "!rm -rf .git\n",
        "!git clone https://github.com/google-research/scenic.git .\n",
        "!python -m pip install -q .\n",
        "!python -m pip install -r ./scenic/projects/owl_vit/requirements.txt\n",
        "\n",
        "# Also install big_vision, which is needed for the mask head:\n",
        "!mkdir /big_vision\n",
        "!git clone https://github.com/google-research/big_vision.git /big_vision\n",
        "!python -m pip install -r /big_vision/big_vision/requirements.txt\n",
        "import sys\n",
        "sys.path.append('/big_vision/')\n",
        "!echo \"Done.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9MKZb6G3-H92"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "from bokeh import io as bokeh_io\n",
        "import jax\n",
        "from google.colab import output as colab_output \n",
        "import matplotlib as mpl\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "from scenic.projects.owl_vit import models\n",
        "from scenic.projects.owl_vit.configs import clip_b16 as config_module\n",
        "from scenic.projects.owl_vit.notebooks import inference\n",
        "from scenic.projects.owl_vit.notebooks import interactive\n",
        "from scenic.projects.owl_vit.notebooks import plotting\n",
        "from scipy.special import expit as sigmoid\n",
        "import skimage\n",
        "from skimage import io as skimage_io\n",
        "from skimage import transform as skimage_transform\n",
        "import tensorflow as tf\n",
        "\n",
        "tf.config.experimental.set_visible_devices([], 'GPU')\n",
        "bokeh_io.output_notebook(hide_banner=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EnD94y6ia6Mn"
      },
      "source": [
        "# Set up the model\n",
        "This takes a minute or two."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1UiX2Nx8auW4"
      },
      "outputs": [],
      "source": [
        "config = config_module.get_config(init_mode='canonical_checkpoint')\n",
        "module = models.TextZeroShotDetectionModule(\n",
        "    body_configs=config.model.body,\n",
        "    normalize=config.model.normalize,\n",
        "    box_bias=config.model.box_bias)\n",
        "variables = module.load_variables(config.init_from.checkpoint_path)\n",
        "model = inference.Model(config, module, variables)\n",
        "model.warm_up()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0Kckjo-Z7nr"
      },
      "source": [
        "# Load example images\n",
        "\n",
        "Please provide a path to a directory containing example images. Google Cloud Storage and local storage are supported."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PmdvY7AEZ9dK"
      },
      "outputs": [],
      "source": [
        "IMAGE_DIR = 'gs://scenic-bucket/owl_vit/example_images'  # @param {\"type\": \"string\"}\n",
        "%matplotlib inline\n",
        "\n",
        "images = {}\n",
        "\n",
        "for i, filename in enumerate(tf.io.gfile.listdir(IMAGE_DIR)):\n",
        "  with tf.io.gfile.GFile(os.path.join(IMAGE_DIR, filename), 'rb') as f:\n",
        "    image = mpl.image.imread(\n",
        "        f, format=os.path.splitext(filename)[-1])[..., :3]\n",
        "  if np.max(image) \u003c= 1.:\n",
        "    image *= 255\n",
        "  images[i] = image\n",
        "\n",
        "cols = 5\n",
        "rows = max(len(images) // 5, 1)\n",
        "fig, axs = plt.subplots(rows, cols, figsize=(16, 8 * rows))\n",
        "\n",
        "for ax in axs.ravel():\n",
        "  ax.set_visible(False)\n",
        "\n",
        "for ax, (ind, image) in zip(axs.ravel(), images.items()):\n",
        "  ax.set_visible(True)\n",
        "  ax.imshow(image)\n",
        "  ax.set_xticks([])\n",
        "  ax.set_yticks([])\n",
        "  ax.set_title(f'Image ID: {ind}')\n",
        "\n",
        "fig.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNzcyP1sbJ9w"
      },
      "source": [
        "# Text-conditioned detection\n",
        "Enter comma-separated queries int the text box above the image to detect stuff. If nothing happens, try running the cell first (\u003ckbd\u003eCtrl\u003c/kbd\u003e+\u003ckbd\u003eEnter\u003c/kbd\u003e)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8teG83eKbNKl"
      },
      "outputs": [],
      "source": [
        "#@title { run: \"auto\" }\n",
        "IMAGE_ID =   2# @param {\"type\": \"number\"}\n",
        "image = images[IMAGE_ID]\n",
        "_, _, boxes = model.embed_image(image)\n",
        "plotting.create_text_conditional_figure(\n",
        "    image=model.preprocess_image(image), boxes=boxes, fig_size=900)\n",
        "interactive.register_text_input_callback(model, image, colab_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFlZhrDTQbiY"
      },
      "source": [
        "# Image-conditioned detection\n",
        "\n",
        "In image-conditioned detection, the model is tasked to detect objects that match a given example image. In the cell below, the example image is chosen by drawing a bounding box around an object in the left image. The model will then detect similar objects in the right image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "AQGAM16fReow"
      },
      "outputs": [],
      "source": [
        "#@title { run: \"auto\" }\n",
        "\n",
        "#@markdown The *query image* is used to select example objects:\n",
        "QUERY_IMAGE_ID = 1  # @param {\"type\": \"number\"}\n",
        "\n",
        "#@markdown Objects will be detected in the *target image* :\n",
        "TARGET_IMAGE_ID = 0  # @param {\"type\": \"number\"}\n",
        "\n",
        "#@markdown Threshold for the minimum confidence that a detection must have to\n",
        "#@markdown be displayed (higher values mean fewer boxes will be shown):\n",
        "MIN_CONFIDENCE = 0.6 #@param { type: \"slider\", min: 0.0, max: 1.0, step: 0.05}\n",
        "\n",
        "\n",
        "#@markdown Threshold for non-maximum suppression of overlapping boxes (higher\n",
        "#@markdown values mean more boxes will be shown):\n",
        "NMS_THRESHOLD = 0.3 #@param { type: \"slider\", min: 0.05, max: 1.0, step: 0.05}\n",
        "\n",
        "interactive.IMAGE_COND_MIN_CONF = MIN_CONFIDENCE\n",
        "interactive.IMAGE_COND_NMS_IOU_THRESHOLD = NMS_THRESHOLD\n",
        "\n",
        "query_image = images[QUERY_IMAGE_ID]\n",
        "target_image = images[TARGET_IMAGE_ID]\n",
        "_, _, boxes = model.embed_image(target_image)\n",
        "plotting.create_image_conditional_figure(\n",
        "    query_image=model.preprocess_image(query_image), \n",
        "    target_image=model.preprocess_image(target_image), \n",
        "    target_boxes=boxes, fig_size=600)\n",
        "interactive.register_box_selection_callback(model, query_image, target_image, colab_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fhPi4XkB3flc"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "EnD94y6ia6Mn"
      ],
      "last_runtime": {
        "build_target": "//learning/deepmind/public/tools/ml_python:ml_notebook",
        "kind": "private"
      },
      "name": "OWL-ViT inference playground.ipynb",
      "private_outputs": true,
      "provenance": [
        {
          "file_id": "1kBebGRuMcABXiprw6IEAxpbKAXNO4EOQ",
          "timestamp": 1651575080312
        },
        {
          "file_id": "https://github.com/google-research/scenic/blob/main/scenic/common_lib/colabs/scenic_playground.ipynb",
          "timestamp": 1650960476931
        }
      ]
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
